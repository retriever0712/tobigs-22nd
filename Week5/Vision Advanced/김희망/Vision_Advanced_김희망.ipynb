{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ARB6yuk9zpD"
      },
      "source": [
        "## ToBigs 5주차"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7NzhIu69zpG"
      },
      "source": [
        "### Vision Advanced 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKlzYdE49zpG"
      },
      "source": [
        "#### 문제 1.\n",
        "\n",
        "Object Detection 에는 2-stage model 과 1-stage model이 존재합니다.  \n",
        "각 유형에 해당하는 model을 하나씩 선정하여 설명하세요. (단, 세션 시간에 설명한 모델과 아래 제시된 모델은 제외할 것)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ3sbOc19zpG"
      },
      "outputs": [],
      "source": [
        "#### 답안 작성\n",
        "\n",
        "# 1-stage model: RetinaNet\n",
        "\n",
        "## 클래스 불균형 문제를 해결하기 위해 Focal Loss를 도입한 것을 특징으로 한다. 다른 모델과 기본적인 골자는 똑같지만 분류 문제를 더욱 잘 해결하기 위해 다른 모델과 다른 loss function을 활용하는게 특징이다.\n",
        "## 클래스 불균형 문제는 사진에는 객체보다 배경이 더 많기 때문에 배경의 class를 예측하기는 상대적으로 쉽지만 객체를 예측하는 것은 어렵기에 학습이 배경에 집중되는 문제이다.\n",
        "## Focal loss는 배경과 같은 쉬운 예측은 가중치를 낮추고 객체와 같이 어려운 예측의 가중치를 높임으로써 객체 탐지에 대한 성능을 향상할 수 있다.\n",
        "\n",
        "# 2-stage model: Mask R-CNN\n",
        "\n",
        "## Mask R-CNN는 객체의 경계를 픽셀 단위까지 구분(Instance Segmentation)하기 위해 Mask Branch라는 과정을 거친다.\n",
        "## 기존 방식대로 객체가 있을 것이라 추정되는 Region Proposal을 특정 크기의 feature map으로 변환 후 해당 지역의 객체가 무엇일지 분류한다.\n",
        "## 이후 해당 영역에서 픽셀 단위 영역마다 분류된 객체가 속하는지 탐지하여 결과적으로 객체의 경계를 세밀하게 탐지한다.\n",
        "## 이때 픽셀마다 Binary Mask(객체가 속하면 1, 아니면 0) 형식으로 값을 반환하며 1이면 해당 픽셀이 예측한 객체가 속할 것으로 판단하고 0이면 배경으로 판단한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvw64DOH9zpH"
      },
      "source": [
        "#### 문제 2.\n",
        "\n",
        "아래 제시된 FasterRCNN 과 YOLOv5 를 각각 실행합니다.  \n",
        "실행 결과를 제시하고 두 모델 사이의 차이점을 실행 결과에 근거하여 설명하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k_uN8DZs9zpH"
      },
      "outputs": [],
      "source": [
        "## Package Import\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK4_kHM1ALw0"
      },
      "outputs": [],
      "source": [
        "## Data Download\n",
        "\n",
        "import os\n",
        "os.makedirs('./data/', exist_ok=True)\n",
        "os.makedirs('./data/images/', exist_ok=True)\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip -d ./data/\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip val2017.zip -d ./data/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kQzE9KrL9zpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ebb851-85d1-422f-f262-ea4986d085cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.14s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 86.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소요 시간: 58.27초\n",
            "정확도: 20.46%\n"
          ]
        }
      ],
      "source": [
        "## FasterRCNN\n",
        "\n",
        "# 경로 설정\n",
        "image_dir = \"./data/images/val2017/\"\n",
        "json_path = \"./data/annotations/instances_val2017.json\"\n",
        "\n",
        "# Transform 설정\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset과 DataLoader 설정\n",
        "dataset = CocoDetection(root=image_dir, annFile=json_path, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# 모델 로드 및 설정\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# 모델의 클래스 수를 3개로 설정 (배경 포함해서 0, 2, background)\n",
        "num_classes = 3  # COCO의 경우 background 포함 (0, 2 두 개의 클래스 + 배경)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# GPU 사용 가능 시 GPU로 설정\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# 모델을 평가 모드로 설정\n",
        "model.eval()\n",
        "\n",
        "# 정확도 계산을 위한 변수\n",
        "total_correct = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# 소요 시간 측정 시작\n",
        "start_time = time.time()\n",
        "\n",
        "# 100장 예측 속도를 비교\n",
        "cnt = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in data_loader:\n",
        "        cnt += 1\n",
        "        images = list(image.to(device) for image in images)\n",
        "\n",
        "        # 각 이미지의 타깃을 적절히 변환하여 GPU로 전송\n",
        "        processed_targets = []\n",
        "        for target in targets:\n",
        "            processed_target = {}\n",
        "            processed_target['boxes'] = torch.tensor([ann['bbox'] for ann in target]).to(device)\n",
        "            processed_target['labels'] = torch.tensor([ann['category_id'] for ann in target]).to(device)\n",
        "            processed_targets.append(processed_target)\n",
        "\n",
        "        # 모델 예측\n",
        "        outputs = model(images)\n",
        "\n",
        "        for i, output in enumerate(outputs):\n",
        "            pred_labels = output['labels'].cpu().numpy()\n",
        "            true_labels = processed_targets[i]['labels'].cpu().numpy()\n",
        "\n",
        "            # 예측 수가 실제 라벨 수보다 많은 경우 예측 수를 잘라냄\n",
        "            if len(pred_labels) > len(true_labels):\n",
        "                pred_labels = pred_labels[:len(true_labels)]\n",
        "\n",
        "            # 예측 레이블과 실제 레이블이 얼마나 일치하는지 확인\n",
        "            correct = np.sum(pred_labels == true_labels[:len(pred_labels)])\n",
        "            total_correct += correct\n",
        "            total_predictions += len(true_labels)\n",
        "\n",
        "        if cnt == 100:\n",
        "            break\n",
        "\n",
        "# 소요 시간 측정 종료\n",
        "end_time = time.time()\n",
        "\n",
        "# 소요 시간 및 정확도 출력\n",
        "time_taken = end_time - start_time\n",
        "accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "print(f\"소요 시간: {time_taken:.2f}초\")\n",
        "print(f\"정확도: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "stB0A3MN9zpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f511d2e-3e7f-4786-af13-b618be465fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.80s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 12.8 MB/s eta 0:00:00\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 299.2 MB/s eta 0:00:00\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 2.7s, installed 1 package: ['gitpython>=3.1.30']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2024-9-24 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 118MB/s] \n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소요 시간: 28.63초\n",
            "정확도: 0.28%\n"
          ]
        }
      ],
      "source": [
        "## YOLOv5\n",
        "\n",
        "# 경로 설정\n",
        "image_dir = \"./data/images/val2017/\"\n",
        "json_path = \"./data/annotations/instances_val2017.json\"\n",
        "\n",
        "# Transform 설정\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),  # YOLOv5의 기본 입력 크기\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset과 DataLoader 설정\n",
        "dataset = CocoDetection(root=image_dir, annFile=json_path, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "# 모델 로드 (PyTorch Hub에서 COCO로 사전 학습된 YOLOv5 모델 로드)\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# GPU 사용 가능 시 GPU로 설정\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 정확도 계산을 위한 변수\n",
        "total_correct = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# 소요 시간 측정 시작\n",
        "start_time = time.time()\n",
        "\n",
        "# 100장 예측 속도를 비교\n",
        "cnt = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in data_loader:\n",
        "        cnt += 1\n",
        "        images = images.to(device)\n",
        "\n",
        "        # 모델 예측\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 예측된 바운딩 박스와 클래스 정보 추출\n",
        "        for output in outputs:\n",
        "            pred_labels = output[:, 5:].argmax(1).cpu().numpy()  # 예측된 클래스 레이블\n",
        "\n",
        "            # 실제 라벨을 추출하고, 예측된 라벨과 비교\n",
        "            true_labels = [t['category_id'].item() for t in targets]\n",
        "\n",
        "            # 예측된 라벨과 실제 라벨이 일치하는지 확인\n",
        "            correct = np.sum(pred_labels[:len(true_labels)] == true_labels)\n",
        "            total_correct += correct\n",
        "            total_predictions += len(true_labels)\n",
        "\n",
        "        if cnt == 100:\n",
        "            break\n",
        "\n",
        "# 소요 시간 측정 종료\n",
        "end_time = time.time()\n",
        "\n",
        "# 소요 시간 및 정확도 출력\n",
        "time_taken = end_time - start_time\n",
        "\n",
        "# 정확도 계산\n",
        "if total_predictions > 0:\n",
        "    accuracy = (total_correct / total_predictions) * 100\n",
        "else:\n",
        "    accuracy = 0\n",
        "\n",
        "# 소요 시간 및 정확도 출력\n",
        "print(f\"소요 시간: {time_taken:.2f}초\")\n",
        "print(f\"정확도: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FasterRCNN: 소요 시간: 58.27초, 정확도: 20.46%\n",
        "\n",
        "### YOLOv5: 소요 시간: 28.63초, 정확도: 0.28%\n",
        "\n",
        "#### FasterRCNN의 경우 Two-stage 모델이기 때문에 후보 영역을 먼저 선발한 뒤에 이 영역을 세밀하게 분석하므로 정확도가 높지만 소요 시간 역시 많이 걸리게 된다. 그러나 YOLOv5의 경우 One-stage 모델이므로 한 번에 모든 처리를 완료하여 시간이 상대적으로 적게 걸리지만 정확도 역시 낮아지는 모습을 확인할 수 있다."
      ],
      "metadata": {
        "id": "caa4KO2if8LY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}