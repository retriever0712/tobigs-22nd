{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:\n",
        "\n",
        " self attention은 모든 단어들이 전부 한 번씩 Query가 되는 것이라고 할 수 있다.\n",
        "\n",
        " attention의 결과는 attention value(실시간 context vector), 즉 Query의 정보가 반영된 key 정보의 가중치 합이다.\n",
        "\n",
        " 모든 단어들이 전부 한 번씩 쿼리가 되어 context embedding을 함으로써, 각 단어의 의미를 그 자신을 포함한 모든 단어들의 영향을 바탕으로 결정하는 것이다. 즉, 그 단어 하나만의 사전적 의미가 아닌, 문장 내 맥락에서 가지는 의미도 포함하는 context vector를 시퀀스마다 실시간으로 만드는 것이다."
      ],
      "metadata": {
        "id": "ojf8hcSd9hLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "fdbd59e4-83e0-4f81-ed4d-3b431844b1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 간단한 문장: ['나는', '사과를', '먹었다']\n",
        "words = ['나는', '사과를', '먹었다']\n",
        "word_vectors = {\n",
        "    '나는': np.array([1, 0, 0]),\n",
        "    '사과를': np.array([0, 1, 0]),\n",
        "    '먹었다': np.array([0, 0, 1])\n",
        "}\n",
        "\n",
        "# 유사도 계산 함수 (self-attention)\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key) # 쿼리와 키의 내적을 통해 유사도를 계산한다.\n",
        "\n",
        "attention_matrix = np.zeros((len(words), len(words)))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        ''' words의 각 요소(토큰)이 쿼리이자 키로 쓰인다.\n",
        "        이중 for문을 통해 모든 단어 쌍에 대해 한 번씩 내적하여,\n",
        "        self attention(자기 자신을 포함한 모든 단어들의 영향을 반영)을 수행한다.'''\n",
        "\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:\n",
        "\n",
        " Multi-Head Self Attention은 encoder 내 각 단어에 대해서 self attention을 동시에 여러 번 수행하는 것(중의성 해소를 위해)으로, 여러 개의 head가 병렬적으로 self attention을 수행한다."
      ],
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 개의 attention heads\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)] # head 개수는 3개가 기본값\n",
        "\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        ''' multi_head_attention_matrix 함수에 포함된 for 문에 의해\n",
        "        multi_head_attention_matrix[i][j]에는 3개의 어텐션 값이 저장된다.\n",
        "        이 값은 i번째 단어와 j번째 단어 간의 attention 값으로,\n",
        "        각 헤드별로 계산된 결과를 개별적으로 저장하고 있다.'''\n",
        "\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "abb4c8a5-2c17-4dc2-9e43-2d7965592d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Masked Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:\n",
        "\n",
        " t시점에서 decoder 내 self attention을 구할 때 t+1 시점부터의 정보는 활용하지 않는 것이다. 디코더는 실시간으로 번역 중인 상황이기 때문에 미래 시점의 정보는 참고할 수 없기 때문이다."
      ],
      "metadata": {
        "id": "lHm1Y03S-hz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스크 추가: 현재 단어 이후의 단어는 계산하지 않음\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask\n",
        "\n",
        "mask = np.array([1, 1, 0])  # 첫 번째, 두 번째는 보지만, 세 번째는 ______ (이 빈칸 꼭 채워주세요 : 보지 않는다.) )\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "        '''내적한 후(어텐션 값을 구한 후) mask와 요소곱을 수행하여 현재 단어 이후의 단어(3번째)는 보지 않는다.'''\n",
        "\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "fcdc9f65-e497-4e06-bf4a-38990feacbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:\n",
        "\n",
        " query는 디코더 쪽의 것, value와 key는 인코더 쪽의 것을 활용하여 attention을 수행하는 것이다. 즉, 인코더와 디코더를 교차하여 attention을 수행한다."
      ],
      "metadata": {
        "id": "wEiAlmYi-xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 문장과 응답 문장\n",
        "question_words = ['너는', '사과를']\n",
        "answer_words = ['나는', '먹었다']\n",
        "question_vectors = {\n",
        "    '너는': np.array([1, 0]),\n",
        "    '사과를': np.array([0, 1])\n",
        "}\n",
        "answer_vectors = {\n",
        "    '나는': np.array([1, 0]),\n",
        "    '먹었다': np.array([0, 1])\n",
        "}\n",
        "\n",
        "# Cross-Attention\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "for i in range(len(question_words)):\n",
        "    for j in range(len(answer_words)):\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n",
        "        '''cross_attention_matrix[i][j]는 question_words의 i번째 단어(인코더)와\n",
        "        answer_words의 j번째 단어(디코더) 사이의 어텐션 값(내적 결과)을 저장한다.\n",
        "        입력 문장과 응답 문장 간의 유사도를 계산하는 과정이다.'''\n",
        "\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "43045e9f-e339-4fd9-db23-fbb5440aeec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}