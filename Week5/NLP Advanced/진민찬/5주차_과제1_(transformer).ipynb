{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojf8hcSd9hLC"
      },
      "source": [
        "**1. Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:\n",
        "Self-Attention은 입력 데이터의 각 요소가 다른 요소들과의 연관성을 고려하여 중요도를 계산하는 메커니즘입니다. 자연어 처리에서 Self-Attention은 단어 간의 관계를 고려하여 문장의 의미를 더 잘 파악하는 데 사용됩니다. 이를 통해 각 단어가 문장 내에서 어느 정도 중요한지를 측정하고, 중요한 단어들에 더 많은 가중치를 부여합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "7c980500-9ada-4d73-e594-d91b876c0b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 간단한 문장: ['나는', '사과를', '먹었다']\n",
        "# 각 단어를 3차원 벡터로 표현한 후 '나는', '사과를', '먹었다' 각각의 벡터값을 설정\n",
        "words = ['나는', '사과를', '먹었다']\n",
        "word_vectors = {\n",
        "    '나는': np.array([1, 0, 0]),   # '나는'을 [1, 0, 0]으로 벡터화\n",
        "    '사과를': np.array([0, 1, 0]),  # '사과를'을 [0, 1, 0]으로 벡터화\n",
        "    '먹었다': np.array([0, 0, 1])   # '먹었다'를 [0, 0, 1]으로 벡터화\n",
        "}\n",
        "\n",
        "# 유사도 계산 함수 (self-attention)\n",
        "# query와 key의 벡터 간 내적(dot product)을 계산하여 두 단어 간 유사도를 구함\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key)  # 두 벡터의 내적을 통해 유사도 계산\n",
        "\n",
        "# Self-Attention Matrix를 저장할 3x3 크기의 행렬을 초기화\n",
        "attention_matrix = np.zeros((len(words), len(words)))  \n",
        "\n",
        "# 모든 단어 쌍에 대해 유사도를 계산하여 Attention Matrix에 채움\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "\n",
        "# 결과로 Self-Attention Matrix 출력\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      },
      "source": [
        "**2. Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:Multi-Head Self-Attention은 Self-Attention의 확장된 버전으로, 여러 개의 독립적인 attention heads를 통해 단어 간의 관계를 동시에 여러 관점에서 파악하는 메커니즘이다. 각 head는 서로 다른 가중치 행렬을 사용하여 입력 데이터를 변환하고, 이를 통해 서로 다른 정보를 추출한 후 결과를 결합하여 보다 풍부한 표현을 얻는다. 이를 통해 모델이 더 다양한 패턴을 학습할 수 있게 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "abb4c8a5-2c17-4dc2-9e43-2d7965592d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ],
      "source": [
        "# 여러 개의 attention heads\n",
        "# query와 key 벡터 간의 내적을 각 head에서 반복 수행\n",
        "# heads 매개변수를 통해 head의 개수를 설정 (기본값은 3)\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)]  # heads 수만큼 내적을 수행하여 결과를 리스트로 반환\n",
        "\n",
        "# 3개의 heads를 가진 Multi-Head Self-Attention Matrix를 저장할 3D 행렬을 초기화\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))  # (단어 수, 단어 수, head 수) 크기의 행렬 생성\n",
        "\n",
        "# 모든 단어 쌍에 대해 Multi-Head Self-Attention을 수행\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "\n",
        "# 결과로 Multi-Head Self-Attention Matrix 출력\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHm1Y03S-hz9"
      },
      "source": [
        "**3. Masked Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:Masked Multi-Head Self-Attention은 Self-Attention에서 특정 단어를 무시하거나 보지 않도록 마스크(mask)를 적용하는 메커니즘이다. 이는 주로 자연어 처리에서 시퀀스 데이터를 다룰 때, 미래의 정보를 미리 참고하지 않도록 하는 데 사용된다. 예를 들어, 언어 모델에서는 현재 단어까지의 정보만을 사용하여 다음 단어를 예측하는 방식으로 동작하므로, 미래의 단어들은 계산에서 제외된다. 이때 Masked Attention을 사용하여 미래의 단어는 무시하고 현재까지의 정보만을 처리하게 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "fcdc9f65-e497-4e06-bf4a-38990feacbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# 마스크 추가: 현재 단어 이후의 단어는 계산하지 않음\n",
        "# mask를 적용하여 이후 단어들은 무시하고 attention 계산\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask  # query와 key의 내적에 mask를 곱하여 마스크된 값을 반환\n",
        "\n",
        "# 마스크 설정: 첫 번째, 두 번째는 보지만, 세 번째는 보지 않음 (즉, 세 번째 단어는 무시)\n",
        "mask = np.array([1, 1, 0])  # 첫 번째, 두 번째 단어는 보지만, 세 번째는 보지 않음\n",
        "\n",
        "# Masked Self-Attention Matrix를 저장할 행렬을 초기화\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
        "\n",
        "# 모든 단어 쌍에 대해 Masked Self-Attention을 수행\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "\n",
        "# 결과로 Masked Self-Attention Matrix 출력\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEiAlmYi-xg9"
      },
      "source": [
        "**4. Cross Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:Cross Attention은 두 개의 서로 다른 시퀀스 사이에서 상호 연관성을 계산하는 메커니즘이다. 일반적으로 하나의 시퀀스는 \"질문\"과 같은 입력 문장을 나타내고, 다른 시퀀스는 \"응답\" 또는 출력 문장을 나타낸다. 이 메커니즘은 입력과 출력 사이의 관련성을 계산하여 출력에서 각 요소가 입력의 어느 부분에 주목해야 하는지를 결정하는 데 사용된다. 이는 특히 번역 시스템이나 질문-응답 시스템과 같은 작업에서 많이 사용된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "43045e9f-e339-4fd9-db23-fbb5440aeec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# 입력 문장과 응답 문장\n",
        "# 질문 문장: ['너는', '사과를']\n",
        "# 응답 문장: ['나는', '먹었다']\n",
        "question_words = ['너는', '사과를']\n",
        "answer_words = ['나는', '먹었다']\n",
        "\n",
        "# 질문 문장에 대한 벡터 정의\n",
        "question_vectors = {\n",
        "    '너는': np.array([1, 0]),   # '너는'을 [1, 0]으로 벡터화\n",
        "    '사과를': np.array([0, 1])  # '사과를'을 [0, 1]으로 벡터화\n",
        "}\n",
        "\n",
        "# 응답 문장에 대한 벡터 정의\n",
        "answer_vectors = {\n",
        "    '나는': np.array([1, 0]),   # '나는'을 [1, 0]으로 벡터화\n",
        "    '먹었다': np.array([0, 1])  # '먹었다'를 [0, 1]으로 벡터화\n",
        "}\n",
        "\n",
        "# Cross-Attention Matrix 초기화 (질문 단어 x 응답 단어 크기)\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "\n",
        "# 모든 질문 단어와 응답 단어 쌍에 대해 유사도(내적)를 계산\n",
        "for i in range(len(question_words)):\n",
        "    for j in range(len(answer_words)):\n",
        "        # 질문 단어와 응답 단어의 벡터 간 내적을 통해 유사도를 계산하고 저장\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n",
        "\n",
        "# 결과로 Cross-Attention Matrix 출력\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
