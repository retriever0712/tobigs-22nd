{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1. Self Attention📌**\n","\n","아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: 각 단어마다 Query로 설정한 다음 다른 단어와 유사도를 계산하는 단계이다."],"metadata":{"id":"ojf8hcSd9hLC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2KyObPG9Z7v","outputId":"7c980500-9ada-4d73-e594-d91b876c0b2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Self-Attention Matrix:\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n"]}],"source":["import numpy as np\n","\n","# 간단한 문장: ['나는', '사과를', '먹었다']\n","words = ['나는', '사과를', '먹었다']\n","word_vectors = {\n","    '나는': np.array([1, 0, 0]),\n","    '사과를': np.array([0, 1, 0]),\n","    '먹었다': np.array([0, 0, 1])\n","}\n","\n","# 유사도 계산 함수 (self-attention)\n","def self_attention(query, key):\n","    return np.dot(query, key)#유사도 계산을 통해 weight를 구함\n","\n","attention_matrix = np.zeros((len(words), len(words)))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","      #자기 자신을 query로 설정하고 다른 벡터와 유사도 계산(value에 곱해질 가중치)\n","        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n","\n","print(\"Self-Attention Matrix:\")\n","print(attention_matrix)\n"]},{"cell_type":"markdown","source":["**2. Multi-Head Self Attention📌**\n","\n","아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: 여러 개의 multi-head를 이용해 서로 다른 self-attention을 병렬로 사용하는 것."],"metadata":{"id":"WA3NEBQC-Dpg"}},{"cell_type":"code","source":["# 여러 개의 attention heads\n","#3개의 head에 관해 query와 key의 dot product를 진행해 리스트로 반환\n","def multi_head_self_attention(query, key, heads=3):\n","    return [np.dot(query, key) for _ in range(heads)]\n","\n","#각각의 단어에 대해 3번 attention 진행\n","multi_head_attention_matrix = np.zeros((len(words), len(words), 3))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","      #self attention\n","        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n","\n","print(\"\\nMulti-Head Self-Attention Matrix:\")\n","print(multi_head_attention_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1bvP6lC-efR","outputId":"abb4c8a5-2c17-4dc2-9e43-2d7965592d86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Multi-Head Self-Attention Matrix:\n","[[[1. 1. 1.]\n","  [0. 0. 0.]\n","  [0. 0. 0.]]\n","\n"," [[0. 0. 0.]\n","  [1. 1. 1.]\n","  [0. 0. 0.]]\n","\n"," [[0. 0. 0.]\n","  [0. 0. 0.]\n","  [1. 1. 1.]]]\n"]}]},{"cell_type":"markdown","source":["**3. Masked Multi-Head Self Attention📌**\n","\n","아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: mask를 통해 특정 부분을 가린채로 attention을 진행하는 것이다"],"metadata":{"id":"lHm1Y03S-hz9"}},{"cell_type":"code","source":["# 마스크 추가: 현재 단어 이후의 단어는 계산하지 않음\n","#mask=0인 경우 해당 단어를 제외하고 부분만 attention 진행\n","def masked_attention(query, key, mask):\n","    return np.dot(query, key) * mask\n","\n","mask = np.array([1, 1, 0])  # 첫 번째, 두 번째는 보지만, 세 번째는 가림 (이 빈칸 꼭 채워주세요 :) )\n","masked_attention_matrix = np.zeros((len(words), len(words)))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n","\n","print(\"\\nMasked Self-Attention Matrix:\")\n","print(masked_attention_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hELE_Nyf-pOP","outputId":"fcdc9f65-e497-4e06-bf4a-38990feacbde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Masked Self-Attention Matrix:\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["**4. Cross Attention📌**\n","\n","아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명:"],"metadata":{"id":"wEiAlmYi-xg9"}},{"cell_type":"code","source":["# 입력 문장과 응답 문장\n","question_words = ['너는', '사과를']\n","answer_words = ['나는', '먹었다']\n","question_vectors = {\n","    '너는': np.array([1, 0]),\n","    '사과를': np.array([0, 1])\n","}\n","answer_vectors = {\n","    '나는': np.array([1, 0]),\n","    '먹었다': np.array([0, 1])\n","}\n","\n","# Cross-Attention\n","cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n","for i in range(len(question_words)):\n","    for j in range(len(answer_words)):\n","        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n","\n","print(\"\\nCross-Attention Matrix:\")\n","print(cross_attention_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2871Q9r-5ZP","outputId":"43045e9f-e339-4fd9-db23-fbb5440aeec4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Cross-Attention Matrix:\n","[[1. 0.]\n"," [0. 1.]]\n"]}]}]}