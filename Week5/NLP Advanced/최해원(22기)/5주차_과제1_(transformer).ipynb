{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1. Self Attention📌**\n","\n","아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: 단어 하나가 query가 되고 나머지가 key가 되어, query와 각 key 사이의 연관성을 내적을 통해 계산한다."],"metadata":{"id":"ojf8hcSd9hLC"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2KyObPG9Z7v","outputId":"2e119665-512c-417b-b4ad-92efafe14ebb","executionInfo":{"status":"ok","timestamp":1727003204680,"user_tz":-540,"elapsed":318,"user":{"displayName":"최해원","userId":"14793508175590533527"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Self-Attention Matrix:\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n"]}],"source":["import numpy as np\n","\n","# 간단한 문장: ['나는', '사과를', '먹었다']\n","words = ['나는', '사과를', '먹었다']\n","word_vectors = {\n","    '나는': np.array([1, 0, 0]),\n","    '사과를': np.array([0, 1, 0]),\n","    '먹었다': np.array([0, 0, 1])\n","}\n","\n","# 유사도 계산 함수 (self-attention)\n","def self_attention(query, key):\n","    return np.dot(query, key) # 벡터 내적으로 연관성 계산\n","\n","attention_matrix = np.zeros((len(words), len(words)))\n","for i in range(len(words)):      # 모든 단어가 한번씩 query가 됨\n","    for j in range(len(words)):  # 각 단어들이 key가 되어\n","        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]]) # 연관성 계산 후 해당하는 행렬 위치에 추가\n","\n","print(\"Self-Attention Matrix:\")\n","print(attention_matrix)\n"]},{"cell_type":"markdown","source":["**2. Multi-Head Self Attention📌**\n","\n","아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: attention을 head의 개수 만큼 나누어 병렬적으로 수행"],"metadata":{"id":"WA3NEBQC-Dpg"}},{"cell_type":"code","source":["# 여러 개의 attention heads\n","def multi_head_self_attention(query, key, heads=3):\n","    return [np.dot(query, key) for _ in range(heads)]  # 연관성 계산 시 head 수 만큼 계산\n","\n","multi_head_attention_matrix = np.zeros((len(words), len(words), 3)) # head 수 만큼\n","for i in range(len(words)):      # 모든 단어가 한번씩 query가 됨\n","    for j in range(len(words)):  # 각 단어들이 key가 되어\n","        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n","        # self attention 적용. head 수 만큼 단어의 관계를 다각적으로 분석\n","\n","print(\"\\nMulti-Head Self-Attention Matrix:\")\n","print(multi_head_attention_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1bvP6lC-efR","outputId":"d45f8293-b044-4420-e695-63a026a271f2","executionInfo":{"status":"ok","timestamp":1727003879830,"user_tz":-540,"elapsed":369,"user":{"displayName":"최해원","userId":"14793508175590533527"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Multi-Head Self-Attention Matrix:\n","[[[1. 1. 1.]\n","  [0. 0. 0.]\n","  [0. 0. 0.]]\n","\n"," [[0. 0. 0.]\n","  [1. 1. 1.]\n","  [0. 0. 0.]]\n","\n"," [[0. 0. 0.]\n","  [0. 0. 0.]\n","  [1. 1. 1.]]]\n"]}]},{"cell_type":"markdown","source":["**3. Masked Multi-Head Self Attention📌**\n","\n","아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명:마스크를 사용하여 특정 단어들의 연관성 분석 제한. 주로 미래 단어들을 참조하지 못하게 차단하는데 사용"],"metadata":{"id":"lHm1Y03S-hz9"}},{"cell_type":"code","source":["# 마스크 추가: 현재 단어 이후의 단어는 계산하지 않음\n","def masked_attention(query, key, mask):\n","    return np.dot(query, key) * mask   # mask 값이 0이면 해당 attetion 값은 계산하지 않음\n","\n","mask = np.array([1, 1, 0])  # 첫 번째, 두 번째는 보지만, 세 번째는 masking (이 빈칸 꼭 채워주세요 :) )\n","masked_attention_matrix = np.zeros((len(words), len(words)))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n","\n","print(\"\\nMasked Self-Attention Matrix:\")\n","print(masked_attention_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hELE_Nyf-pOP","outputId":"1de4c944-b241-4ccf-fd6c-20be0bc15e08","executionInfo":{"status":"ok","timestamp":1727003883144,"user_tz":-540,"elapsed":339,"user":{"displayName":"최해원","userId":"14793508175590533527"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Masked Self-Attention Matrix:\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["**4. Cross Attention📌**\n","\n","아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명:질문과 응답 문장 간의 attention 계산"],"metadata":{"id":"wEiAlmYi-xg9"}},{"cell_type":"code","source":["# 입력 문장과 응답 문장\n","question_words = ['너는', '사과를']\n","answer_words = ['나는', '먹었다']\n","question_vectors = {\n","    '너는': np.array([1, 0]),\n","    '사과를': np.array([0, 1])\n","}\n","answer_vectors = {\n","    '나는': np.array([1, 0]),\n","    '먹었다': np.array([0, 1])\n","}\n","\n","# Cross-Attention\n","cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n","for i in range(len(question_words)):    # 질문 문장의 단어가 query가 되고\n","    for j in range(len(answer_words)):  # 응답 문장의 단어가 key가 되어\n","        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n","        # query와 key의 attention 계산\n","print(\"\\nCross-Attention Matrix:\")\n","print(cross_attention_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2871Q9r-5ZP","outputId":"1ebb45de-c576-4ac5-d3fc-e34b20f47c76","executionInfo":{"status":"ok","timestamp":1727003885702,"user_tz":-540,"elapsed":488,"user":{"displayName":"최해원","userId":"14793508175590533527"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Cross-Attention Matrix:\n","[[1. 0.]\n"," [0. 1.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OmWRXHXqc_zo"},"execution_count":null,"outputs":[]}]}