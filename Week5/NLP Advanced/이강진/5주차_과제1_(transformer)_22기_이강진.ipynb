{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:   Attention중에서 자기 자신에게 Attention 메커니즘을 행하는 방식. (Query = Key = Value)"
      ],
      "metadata": {
        "id": "ojf8hcSd9hLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "ee978a47-c465-4c6a-a7ea-3fd0b8f845a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 간단한 문장: ['나는', '사과를', '먹었다']\n",
        "words = ['나는', '사과를', '먹었다']\n",
        "\n",
        "# word 를 vector 로 embedding\n",
        "word_vectors = {\n",
        "    '나는': np.array([1, 0, 0]),\n",
        "    '사과를': np.array([0, 1, 0]),\n",
        "    '먹었다': np.array([0, 0, 1])\n",
        "}\n",
        "\n",
        "# 유사도 계산 함수 (self-attention)\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key) # query 와 key 내적 (둘 사이 연관성 계산 위해서)\n",
        "\n",
        "attention_matrix = np.zeros((len(words), len(words))) # 위의 내적 값이 attention score 로 저장하기 위해 동일 크기의 0 벡터 생성\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]]) # 각 벡터간 얼마나 유사한지 내적 계산(자기자신과 계산 경우도 있음)\n",
        "\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명: Query, Key, Value값을 한 번에 계산하지 않고 head 수만큼 나눠 계산 후 나중에 Attention Value들을 합치는 메커니즘. 한마디로 분할 계산 후 합산하는 방식.\n",
        "\n",
        "Multi-head Attention\n",
        "\n",
        "1. 원래 Query, Key, Value 행렬 값을 head 수만큼 분할\n",
        "\n",
        "2. 분할된 행렬 값을 통해, 각 Attention value값들을 도출\n",
        "\n",
        "3. 도출된 Attention value값들을 concatenate(쌓아 합치기)하여 최종 Attention value도출"
      ],
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 개의 attention heads\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)]  # 각 헤드에 대해서 attention 계산\n",
        "\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3)) # head 3개 이므로 attention value 저장할 0 매트리스 생성\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "dde5b15d-f4ff-4b26-dfe7-7096577380f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Masked Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명: 디코더(Decoder)에서 사용되는 기법으로, 다중 헤드(Self-Attention)의 변형입니다. 주요 목표는 미래 정보를 차단하여, 모델이 시퀀스를 예측할 때 현재 시점 이후의 단어들을 보지 못하게 하는 것이고, 메커니즘은\n",
        "1. Query, Key, Value 벡터 생성 후,\n",
        "\n",
        "2. 미래 단어들에 대한 마스킹 진행되는 데 마스킹 행렬은 상삼각 행렬(Upper Triangular Matrix) 형태로 이렇게 하면 각 단어가 자신을 포함해 그 이전 단어들에만 Attention을 할 수 있고, 이후의 단어는 무시됩니다.\n",
        "3. Attention Score 계산 및 마스킹 적용: 마스크 행렬에 0 대신 매우 큰 음수를 사용하여 소프트맥스 함수에 의해 해당 가중치가 0으로 되도록 만듭니다. 즉, 미래 단어에 대한 가중치는 0이 되고, 모델은 오직 자신과 그 이전 단어들에만 집중하게 됩니다.\n"
      ],
      "metadata": {
        "id": "lHm1Y03S-hz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스크 추가: 현재 단어 이후의 단어는 계산하지 않음\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask #\n",
        "\n",
        "mask = np.array([1, 1, 0])  # 첫 번째, 두 번째는 보지만, 세 번째는 Mask (이 빈칸 꼭 채워주세요 :) )\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "fabed0f9-215a-4f7c-80a0-910c69d4b1e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명: 디코더의 Query 벡터가 인코더의 Key 및 Value 벡터와 상호작용하는 방식입니다. 즉, 디코더는 자기 자신에 대한 정보뿐만 아니라 인코더가 생성한 문맥 정보를 활용하여 다음 단어를 예측합니다."
      ],
      "metadata": {
        "id": "wEiAlmYi-xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 문장과 응답 문장\n",
        "question_words = ['너는', '사과를']\n",
        "answer_words = ['나는', '먹었다']\n",
        "question_vectors = {\n",
        "    '너는': np.array([1, 0]),\n",
        "    '사과를': np.array([0, 1])\n",
        "}\n",
        "answer_vectors = {\n",
        "    '나는': np.array([1, 0]),\n",
        "    '먹었다': np.array([0, 1])\n",
        "}\n",
        "\n",
        "# Cross-Attention\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "for i in range(len(question_words)):\n",
        "    for j in range(len(answer_words)):\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]]) # question 과 answer 이 상호작용\n",
        "\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "325639d3-f019-4126-a75a-b70f4c8ef5d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTpMpFQpCFxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}