{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daanbee/tobigs-22nd/blob/main/%ED%88%AC%EB%B9%85%EC%8A%A4_Week5_%EA%B3%BC%EC%A0%9C1_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:"
      ],
      "metadata": {
        "id": "ojf8hcSd9hLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "b9d9aade-6f71-4095-a5bd-e2007938c22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 간단한 문장: ['나는', '사과를', '먹었다']\n",
        "words = ['나는', '사과를', '먹었다']  # 문장을 구성하는 단어 리스트\n",
        "\n",
        "# 각 단어에 대한 임의의 벡터 표현 (Word Embedding)\n",
        "word_vectors = {\n",
        "    '나는': np.array([1, 0, 0]),   # '나는'에 해당하는 벡터\n",
        "    '사과를': np.array([0, 1, 0]),  # '사과를'에 해당하는 벡터\n",
        "    '먹었다': np.array([0, 0, 1])   # '먹었다'에 해당하는 벡터\n",
        "}\n",
        "\n",
        "# 유사도 계산 함수 (Self-Attention의 핵심 연산: Query와 Key 간 내적)\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key)  # Query와 Key의 벡터 내적을 통해 유사도 계산\n",
        "\n",
        "# Attention 값을 담을 행렬 초기화 (단어 개수 x 단어 개수 크기)\n",
        "attention_matrix = np.zeros((len(words), len(words)))\n",
        "\n",
        "# 각 단어 간의 Self-Attention 계산 (Query와 Key를 비교하여 Attention 값 저장)\n",
        "for i in range(len(words)):  # i번째 단어를 Query로 설정\n",
        "    for j in range(len(words)):  # j번째 단어를 Key로 설정\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        # i번째 단어(Query)와 j번째 단어(Key)의 유사도를 행렬에 저장\n",
        "\n",
        "# Self-Attention 행렬 출력\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:"
      ],
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 개의 attention heads를 계산하는 함수\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)]  # Query와 Key의 벡터 내적을 heads 개수만큼 계산하여 리스트로 반환\n",
        "\n",
        "# 3개의 Attention Heads를 가지는 Multi-Head Attention 행렬 초기화\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))  # (단어 개수, 단어 개수, heads 개수) 크기의 3D 행렬\n",
        "\n",
        "# 각 단어 쌍에 대해 Multi-Head Self-Attention 계산\n",
        "for i in range(len(words)):  # i번째 단어를 Query로 설정\n",
        "    for j in range(len(words)):  # j번째 단어를 Key로 설정\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        # i번째 단어(Query)와 j번째 단어(Key)에 대해 각 head별 attention 값을 행렬에 저장\n",
        "\n",
        "# Multi-Head Self-Attention 행렬 출력\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "fdf5f51a-73c7-4290-c72a-fbbaeac85160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Masked Multi-Head Self Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:"
      ],
      "metadata": {
        "id": "lHm1Y03S-hz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스크된 Attention을 계산하는 함수 (현재 단어 이후의 단어는 계산하지 않음)\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask  # Query와 Key의 내적 결과에 mask를 곱해 불필요한 계산을 제외\n",
        "\n",
        "# 마스크 배열 (첫 번째, 두 번째 단어는 보지만, 세 번째 단어는 무시)\n",
        "mask = np.array([1, 1, 0])  # 첫 번째, 두 번째 단어는 계산, 세 번째 단어는 계산하지 않음\n",
        "\n",
        "# 마스크된 Self-Attention 행렬 초기화\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))  # 단어 개수 x 단어 개수 크기의 2D 행렬\n",
        "\n",
        "# 각 단어 쌍에 대해 Masked Self-Attention 계산\n",
        "for i in range(len(words)):  # i번째 단어를 Query로 설정\n",
        "    for j in range(len(words)):  # j번째 단어를 Key로 설정\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "        # j번째 단어(Key)에 대해 마스크 값을 적용하여 attention 값을 계산\n",
        "\n",
        "# 마스크된 Self-Attention 행렬 출력\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "205f1774-febf-4e33-ee26-ef08e87e66ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross Attention📌**\n",
        "\n",
        "아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n",
        "\n",
        " 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n",
        "\n",
        " ✅설명:"
      ],
      "metadata": {
        "id": "wEiAlmYi-xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 문장과 응답 문장에 포함된 단어 리스트\n",
        "question_words = ['너는', '사과를']  # 질문에 해당하는 단어 리스트\n",
        "answer_words = ['나는', '먹었다']   # 응답에 해당하는 단어 리스트\n",
        "\n",
        "# 질문 문장에 있는 각 단어의 벡터 표현 (Word Embedding)\n",
        "question_vectors = {\n",
        "    '너는': np.array([1, 0]),   # '너는'에 해당하는 벡터\n",
        "    '사과를': np.array([0, 1])  # '사과를'에 해당하는 벡터\n",
        "}\n",
        "\n",
        "# 응답 문장에 있는 각 단어의 벡터 표현 (Word Embedding)\n",
        "answer_vectors = {\n",
        "    '나는': np.array([1, 0]),   # '나는'에 해당하는 벡터\n",
        "    '먹었다': np.array([0, 1])  # '먹었다'에 해당하는 벡터\n",
        "}\n",
        "\n",
        "# Cross-Attention 행렬 초기화 (질문 단어 개수 x 응답 단어 개수 크기의 행렬)\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "\n",
        "# 질문 단어와 응답 단어 사이의 Cross-Attention 계산\n",
        "for i in range(len(question_words)):  # i번째 질문 단어를 Query로 설정\n",
        "    for j in range(len(answer_words)):  # j번째 응답 단어를 Key로 설정\n",
        "        # 질문 단어의 벡터와 응답 단어의 벡터 간 내적을 통해 Cross-Attention 값을 계산\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n",
        "\n",
        "# Cross-Attention 행렬 출력\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "9cbef301-52d9-4a13-8ac4-ab95a1c9ccd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}
