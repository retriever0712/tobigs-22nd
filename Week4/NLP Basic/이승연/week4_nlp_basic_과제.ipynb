{"cells":[{"cell_type":"markdown","metadata":{"id":"9WW4t3iCYKuy"},"source":["## 과제 1\n","\n","### **Q. 각 모델이 충족하는 속성에 대해 아래 표를 O/X로 채워주세요.**\n","\n","📍5번째 속성은 **LSTM 기준으로** O/X 여부 판단해주세요 ! <br>\n","📍정답은 과제 마감 다음날 (9월 11일 수요일)에 **노션-정규세션-NLP basic**에 업로드 예정\n","\n","\n","> #### **속성 설명**\n","1. Order matters : 입력 시퀀스의 순서 중요 여부\n","2. Variable Length : 고정된 길이가 아닌 다양한 길이의 시퀀스를 처리할 수 있는지 여부\n","3. Differentiable : 미분가능\n","4. Pairwise encoding : 두 단어 사이의 관계를 표현\n","5. Preserves long-term : 장기적인 의존성\n"]},{"cell_type":"markdown","metadata":{"id":"paUeOH0OYNU0"},"source":["|               | N-gram | RNN   | LSTM  | Transformer |\n","|:-------------:|:------:|:-----:|:-----:|:-----------:|\n","| Order matters |   O    |  O  |  O   | O           |\n","| Variable length |   X  |  O  |  O   | O           |\n","| Differentiable |   X   |  O  |  O   | O           |\n","| Pairwise encoding |  X |  X  |  X   | O           |\n","| Preserves long-term | X|  X  |  O   | O           |\n"]},{"cell_type":"markdown","metadata":{"id":"14MthA8WYQev"},"source":["## 과제 2\n","\n","\n","### 목표 : 독일어를 영어로 번역하는 모델 만들기\n","독일어 문장을 입력하면 영어로 번역해주는 모델을 seq2seq로 구현해봅시다"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7PbwGzED6TIV"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchtext==0.6.0\n","  Using cached torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n","Collecting tqdm (from torchtext==0.6.0)\n","  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n","Collecting requests (from torchtext==0.6.0)\n","  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting torch (from torchtext==0.6.0)\n","  Downloading torch-2.4.1-cp38-cp38-win_amd64.whl.metadata (27 kB)\n","Requirement already satisfied: numpy in c:\\users\\win2d\\anaconda3\\envs\\myenv38\\lib\\site-packages (from torchtext==0.6.0) (1.24.4)\n","Requirement already satisfied: six in c:\\users\\win2d\\anaconda3\\envs\\myenv38\\lib\\site-packages (from torchtext==0.6.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6.0)\n","  Downloading sentencepiece-0.2.0-cp38-cp38-win_amd64.whl.metadata (8.3 kB)\n","Collecting charset-normalizer<4,>=2 (from requests->torchtext==0.6.0)\n","  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl.metadata (34 kB)\n","Collecting idna<4,>=2.5 (from requests->torchtext==0.6.0)\n","  Using cached idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests->torchtext==0.6.0)\n","  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n","Collecting certifi>=2017.4.17 (from requests->torchtext==0.6.0)\n","  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n","Collecting filelock (from torch->torchtext==0.6.0)\n","  Using cached filelock-3.16.0-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\win2d\\anaconda3\\envs\\myenv38\\lib\\site-packages (from torch->torchtext==0.6.0) (4.12.2)\n","Collecting sympy (from torch->torchtext==0.6.0)\n","  Using cached sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch->torchtext==0.6.0)\n","  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: jinja2 in c:\\users\\win2d\\anaconda3\\envs\\myenv38\\lib\\site-packages (from torch->torchtext==0.6.0) (3.1.4)\n","Collecting fsspec (from torch->torchtext==0.6.0)\n","  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: colorama in c:\\users\\win2d\\anaconda3\\envs\\myenv38\\lib\\site-packages (from tqdm->torchtext==0.6.0) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\win2d\\anaconda3\\envs\\myenv38\\lib\\site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->torchtext==0.6.0)\n","  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Using cached torchtext-0.6.0-py3-none-any.whl (64 kB)\n","Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n","Downloading sentencepiece-0.2.0-cp38-cp38-win_amd64.whl (991 kB)\n","   ---------------------------------------- 0.0/991.7 kB ? eta -:--:--\n","   ------------------------------- -------- 786.4/991.7 kB 4.8 MB/s eta 0:00:01\n","   ---------------------------------------- 991.7/991.7 kB 5.2 MB/s eta 0:00:00\n","Downloading torch-2.4.1-cp38-cp38-win_amd64.whl (199.4 MB)\n","   ---------------------------------------- 0.0/199.4 MB ? eta -:--:--\n","   ---------------------------------------- 0.8/199.4 MB 4.8 MB/s eta 0:00:42\n","   ---------------------------------------- 1.6/199.4 MB 4.2 MB/s eta 0:00:48\n","    --------------------------------------- 2.6/199.4 MB 4.4 MB/s eta 0:00:45\n","    --------------------------------------- 3.7/199.4 MB 4.4 MB/s eta 0:00:45\n","    --------------------------------------- 4.5/199.4 MB 4.4 MB/s eta 0:00:45\n","   - -------------------------------------- 5.2/199.4 MB 4.3 MB/s eta 0:00:46\n","   - -------------------------------------- 6.3/199.4 MB 4.3 MB/s eta 0:00:46\n","   - -------------------------------------- 7.3/199.4 MB 4.3 MB/s eta 0:00:45\n","   - -------------------------------------- 8.1/199.4 MB 4.3 MB/s eta 0:00:45\n","   - -------------------------------------- 8.9/199.4 MB 4.3 MB/s eta 0:00:45\n","   -- ------------------------------------- 10.2/199.4 MB 4.4 MB/s eta 0:00:44\n","   -- ------------------------------------- 11.3/199.4 MB 4.5 MB/s eta 0:00:42\n","   -- ------------------------------------- 12.6/199.4 MB 4.6 MB/s eta 0:00:41\n","   -- ------------------------------------- 13.6/199.4 MB 4.6 MB/s eta 0:00:41\n","   -- ------------------------------------- 14.9/199.4 MB 4.7 MB/s eta 0:00:40\n","   --- ------------------------------------ 16.0/199.4 MB 4.8 MB/s eta 0:00:39\n","   --- ------------------------------------ 16.8/199.4 MB 4.7 MB/s eta 0:00:39\n","   --- ------------------------------------ 17.6/199.4 MB 4.6 MB/s eta 0:00:40\n","   --- ------------------------------------ 18.6/199.4 MB 4.6 MB/s eta 0:00:40\n","   --- ------------------------------------ 19.4/199.4 MB 4.6 MB/s eta 0:00:39\n","   ---- ----------------------------------- 20.7/199.4 MB 4.7 MB/s eta 0:00:39\n","   ---- ----------------------------------- 22.0/199.4 MB 4.7 MB/s eta 0:00:38\n","   ---- ----------------------------------- 22.8/199.4 MB 4.7 MB/s eta 0:00:38\n","   ---- ----------------------------------- 23.9/199.4 MB 4.8 MB/s eta 0:00:37\n","   ----- ---------------------------------- 25.2/199.4 MB 4.8 MB/s eta 0:00:37\n","   ----- ---------------------------------- 26.2/199.4 MB 4.8 MB/s eta 0:00:36\n","   ----- ---------------------------------- 27.8/199.4 MB 4.9 MB/s eta 0:00:36\n","   ----- ---------------------------------- 29.1/199.4 MB 4.9 MB/s eta 0:00:35\n","   ------ --------------------------------- 30.1/199.4 MB 5.0 MB/s eta 0:00:35\n","   ------ --------------------------------- 31.5/199.4 MB 5.0 MB/s eta 0:00:34\n","   ------ --------------------------------- 32.8/199.4 MB 5.0 MB/s eta 0:00:34\n","   ------ --------------------------------- 34.1/199.4 MB 5.1 MB/s eta 0:00:33\n","   ------- -------------------------------- 35.1/199.4 MB 5.1 MB/s eta 0:00:33\n","   ------- -------------------------------- 37.0/199.4 MB 5.2 MB/s eta 0:00:32\n","   ------- -------------------------------- 38.3/199.4 MB 5.2 MB/s eta 0:00:32\n","   ------- -------------------------------- 39.3/199.4 MB 5.2 MB/s eta 0:00:31\n","   -------- ------------------------------- 40.1/199.4 MB 5.2 MB/s eta 0:00:31\n","   -------- ------------------------------- 40.9/199.4 MB 5.1 MB/s eta 0:00:31\n","   -------- ------------------------------- 41.9/199.4 MB 5.1 MB/s eta 0:00:31\n","   -------- ------------------------------- 43.0/199.4 MB 5.1 MB/s eta 0:00:31\n","   -------- ------------------------------- 44.3/199.4 MB 5.1 MB/s eta 0:00:31\n","   --------- ------------------------------ 45.4/199.4 MB 5.1 MB/s eta 0:00:30\n","   --------- ------------------------------ 45.9/199.4 MB 5.1 MB/s eta 0:00:31\n","   --------- ------------------------------ 46.7/199.4 MB 5.0 MB/s eta 0:00:31\n","   --------- ------------------------------ 47.2/199.4 MB 5.0 MB/s eta 0:00:31\n","   --------- ------------------------------ 48.0/199.4 MB 5.0 MB/s eta 0:00:31\n","   --------- ------------------------------ 49.0/199.4 MB 5.0 MB/s eta 0:00:31\n","   ---------- ----------------------------- 50.1/199.4 MB 5.0 MB/s eta 0:00:31\n","   ---------- ----------------------------- 50.9/199.4 MB 5.0 MB/s eta 0:00:30\n","   ---------- ----------------------------- 51.9/199.4 MB 5.0 MB/s eta 0:00:30\n","   ---------- ----------------------------- 53.0/199.4 MB 5.0 MB/s eta 0:00:30\n","   ---------- ----------------------------- 53.7/199.4 MB 4.9 MB/s eta 0:00:30\n","   ---------- ----------------------------- 54.8/199.4 MB 4.9 MB/s eta 0:00:30\n","   ----------- ---------------------------- 55.6/199.4 MB 4.9 MB/s eta 0:00:30\n","   ----------- ---------------------------- 56.6/199.4 MB 4.9 MB/s eta 0:00:29\n","   ----------- ---------------------------- 57.1/199.4 MB 4.9 MB/s eta 0:00:30\n","   ----------- ---------------------------- 58.2/199.4 MB 4.9 MB/s eta 0:00:29\n","   ----------- ---------------------------- 59.2/199.4 MB 4.9 MB/s eta 0:00:29\n","   ------------ --------------------------- 60.3/199.4 MB 4.9 MB/s eta 0:00:29\n","   ------------ --------------------------- 61.3/199.4 MB 4.9 MB/s eta 0:00:29\n","   ------------ --------------------------- 62.4/199.4 MB 4.9 MB/s eta 0:00:28\n","   ------------ --------------------------- 63.2/199.4 MB 4.9 MB/s eta 0:00:28\n","   ------------ --------------------------- 64.0/199.4 MB 4.9 MB/s eta 0:00:28\n","   ------------- -------------------------- 65.0/199.4 MB 4.9 MB/s eta 0:00:28\n","   ------------- -------------------------- 65.8/199.4 MB 4.8 MB/s eta 0:00:28\n","   ------------- -------------------------- 67.1/199.4 MB 4.9 MB/s eta 0:00:28\n","   ------------- -------------------------- 67.9/199.4 MB 4.9 MB/s eta 0:00:28\n","   ------------- -------------------------- 68.7/199.4 MB 4.8 MB/s eta 0:00:28\n","   ------------- -------------------------- 69.5/199.4 MB 4.8 MB/s eta 0:00:27\n","   -------------- ------------------------- 70.8/199.4 MB 4.8 MB/s eta 0:00:27\n","   -------------- ------------------------- 71.8/199.4 MB 4.8 MB/s eta 0:00:27\n","   -------------- ------------------------- 72.9/199.4 MB 4.8 MB/s eta 0:00:27\n","   -------------- ------------------------- 73.9/199.4 MB 4.8 MB/s eta 0:00:26\n","   --------------- ------------------------ 75.0/199.4 MB 4.8 MB/s eta 0:00:26\n","   --------------- ------------------------ 75.8/199.4 MB 4.8 MB/s eta 0:00:26\n","   --------------- ------------------------ 76.8/199.4 MB 4.8 MB/s eta 0:00:26\n","   --------------- ------------------------ 78.1/199.4 MB 4.9 MB/s eta 0:00:26\n","   --------------- ------------------------ 79.4/199.4 MB 4.9 MB/s eta 0:00:25\n","   ---------------- ----------------------- 80.7/199.4 MB 4.9 MB/s eta 0:00:25\n","   ---------------- ----------------------- 82.1/199.4 MB 4.9 MB/s eta 0:00:24\n","   ---------------- ----------------------- 83.4/199.4 MB 4.9 MB/s eta 0:00:24\n","   ---------------- ----------------------- 84.4/199.4 MB 4.9 MB/s eta 0:00:24\n","   ----------------- ---------------------- 85.7/199.4 MB 4.9 MB/s eta 0:00:24\n","   ----------------- ---------------------- 87.0/199.4 MB 4.9 MB/s eta 0:00:23\n","   ----------------- ---------------------- 88.1/199.4 MB 4.9 MB/s eta 0:00:23\n","   ----------------- ---------------------- 88.9/199.4 MB 4.9 MB/s eta 0:00:23\n","   ------------------ --------------------- 90.4/199.4 MB 5.0 MB/s eta 0:00:22\n","   ------------------ --------------------- 91.2/199.4 MB 5.0 MB/s eta 0:00:22\n","   ------------------ --------------------- 92.5/199.4 MB 5.0 MB/s eta 0:00:22\n","   ------------------ --------------------- 93.8/199.4 MB 5.0 MB/s eta 0:00:22\n","   ------------------- -------------------- 95.2/199.4 MB 5.0 MB/s eta 0:00:21\n","   ------------------- -------------------- 96.2/199.4 MB 5.0 MB/s eta 0:00:21\n","   ------------------- -------------------- 97.8/199.4 MB 5.0 MB/s eta 0:00:21\n","   ------------------- -------------------- 98.8/199.4 MB 5.0 MB/s eta 0:00:21\n","   -------------------- ------------------- 99.9/199.4 MB 5.0 MB/s eta 0:00:20\n","   -------------------- ------------------- 100.7/199.4 MB 5.0 MB/s eta 0:00:20\n","   -------------------- ------------------- 102.0/199.4 MB 5.0 MB/s eta 0:00:20\n","   -------------------- ------------------- 103.3/199.4 MB 5.0 MB/s eta 0:00:20\n","   -------------------- ------------------- 104.6/199.4 MB 5.0 MB/s eta 0:00:19\n","   --------------------- ------------------ 105.9/199.4 MB 5.1 MB/s eta 0:00:19\n","   --------------------- ------------------ 107.0/199.4 MB 5.0 MB/s eta 0:00:19\n","   --------------------- ------------------ 108.0/199.4 MB 5.0 MB/s eta 0:00:19\n","   --------------------- ------------------ 109.1/199.4 MB 5.0 MB/s eta 0:00:18\n","   ---------------------- ----------------- 110.1/199.4 MB 5.0 MB/s eta 0:00:18\n","   ---------------------- ----------------- 111.4/199.4 MB 5.1 MB/s eta 0:00:18\n","   ---------------------- ----------------- 112.7/199.4 MB 5.1 MB/s eta 0:00:18\n","   ---------------------- ----------------- 114.0/199.4 MB 5.1 MB/s eta 0:00:17\n","   ----------------------- ---------------- 115.1/199.4 MB 5.1 MB/s eta 0:00:17\n","   ----------------------- ---------------- 116.4/199.4 MB 5.1 MB/s eta 0:00:17\n","   ----------------------- ---------------- 117.2/199.4 MB 5.1 MB/s eta 0:00:17\n","   ----------------------- ---------------- 118.2/199.4 MB 5.1 MB/s eta 0:00:17\n","   ----------------------- ---------------- 119.3/199.4 MB 5.1 MB/s eta 0:00:16\n","   ------------------------ --------------- 120.1/199.4 MB 5.1 MB/s eta 0:00:16\n","   ------------------------ --------------- 120.8/199.4 MB 5.1 MB/s eta 0:00:16\n","   ------------------------ --------------- 121.9/199.4 MB 5.0 MB/s eta 0:00:16\n","   ------------------------ --------------- 122.9/199.4 MB 5.0 MB/s eta 0:00:16\n","   ------------------------ --------------- 124.3/199.4 MB 5.1 MB/s eta 0:00:15\n","   ------------------------- -------------- 125.6/199.4 MB 5.1 MB/s eta 0:00:15\n","   ------------------------- -------------- 126.9/199.4 MB 5.1 MB/s eta 0:00:15\n","   ------------------------- -------------- 127.9/199.4 MB 5.1 MB/s eta 0:00:15\n","   ------------------------- -------------- 129.0/199.4 MB 5.1 MB/s eta 0:00:14\n","   -------------------------- ------------- 130.3/199.4 MB 5.1 MB/s eta 0:00:14\n","   -------------------------- ------------- 131.6/199.4 MB 5.1 MB/s eta 0:00:14\n","   -------------------------- ------------- 132.9/199.4 MB 5.1 MB/s eta 0:00:14\n","   -------------------------- ------------- 134.2/199.4 MB 5.1 MB/s eta 0:00:13\n","   --------------------------- ------------ 135.5/199.4 MB 5.1 MB/s eta 0:00:13\n","   --------------------------- ------------ 136.3/199.4 MB 5.1 MB/s eta 0:00:13\n","   --------------------------- ------------ 137.4/199.4 MB 5.1 MB/s eta 0:00:13\n","   --------------------------- ------------ 138.4/199.4 MB 5.1 MB/s eta 0:00:12\n","   --------------------------- ------------ 139.5/199.4 MB 5.1 MB/s eta 0:00:12\n","   ---------------------------- ----------- 140.8/199.4 MB 5.1 MB/s eta 0:00:12\n","   ---------------------------- ----------- 141.6/199.4 MB 5.1 MB/s eta 0:00:12\n","   ---------------------------- ----------- 142.6/199.4 MB 5.1 MB/s eta 0:00:12\n","   ---------------------------- ----------- 143.9/199.4 MB 5.1 MB/s eta 0:00:11\n","   ----------------------------- ---------- 145.0/199.4 MB 5.1 MB/s eta 0:00:11\n","   ----------------------------- ---------- 146.0/199.4 MB 5.1 MB/s eta 0:00:11\n","   ----------------------------- ---------- 147.1/199.4 MB 5.1 MB/s eta 0:00:11\n","   ----------------------------- ---------- 148.1/199.4 MB 5.1 MB/s eta 0:00:11\n","   ----------------------------- ---------- 148.9/199.4 MB 5.1 MB/s eta 0:00:10\n","   ------------------------------ --------- 149.9/199.4 MB 5.1 MB/s eta 0:00:10\n","   ------------------------------ --------- 150.7/199.4 MB 5.1 MB/s eta 0:00:10\n","   ------------------------------ --------- 151.5/199.4 MB 5.1 MB/s eta 0:00:10\n","   ------------------------------ --------- 152.6/199.4 MB 5.1 MB/s eta 0:00:10\n","   ------------------------------ --------- 153.4/199.4 MB 5.1 MB/s eta 0:00:10\n","   ------------------------------ --------- 154.1/199.4 MB 5.1 MB/s eta 0:00:09\n","   ------------------------------- -------- 154.7/199.4 MB 5.1 MB/s eta 0:00:09\n","   ------------------------------- -------- 155.7/199.4 MB 5.1 MB/s eta 0:00:09\n","   ------------------------------- -------- 156.2/199.4 MB 5.1 MB/s eta 0:00:09\n","   ------------------------------- -------- 157.0/199.4 MB 5.0 MB/s eta 0:00:09\n","   ------------------------------- -------- 157.3/199.4 MB 5.0 MB/s eta 0:00:09\n","   ------------------------------- -------- 158.1/199.4 MB 5.0 MB/s eta 0:00:09\n","   ------------------------------- -------- 159.1/199.4 MB 5.0 MB/s eta 0:00:09\n","   -------------------------------- ------- 159.9/199.4 MB 5.0 MB/s eta 0:00:08\n","   -------------------------------- ------- 160.7/199.4 MB 5.0 MB/s eta 0:00:08\n","   -------------------------------- ------- 161.7/199.4 MB 5.0 MB/s eta 0:00:08\n","   -------------------------------- ------- 163.1/199.4 MB 5.0 MB/s eta 0:00:08\n","   -------------------------------- ------- 164.4/199.4 MB 5.0 MB/s eta 0:00:08\n","   --------------------------------- ------ 165.4/199.4 MB 5.0 MB/s eta 0:00:07\n","   --------------------------------- ------ 165.7/199.4 MB 5.0 MB/s eta 0:00:07\n","   --------------------------------- ------ 166.7/199.4 MB 5.0 MB/s eta 0:00:07\n","   --------------------------------- ------ 167.5/199.4 MB 5.0 MB/s eta 0:00:07\n","   --------------------------------- ------ 168.0/199.4 MB 5.0 MB/s eta 0:00:07\n","   --------------------------------- ------ 168.6/199.4 MB 5.0 MB/s eta 0:00:07\n","   ---------------------------------- ----- 169.6/199.4 MB 4.9 MB/s eta 0:00:07\n","   ---------------------------------- ----- 170.7/199.4 MB 4.9 MB/s eta 0:00:06\n","   ---------------------------------- ----- 171.4/199.4 MB 4.9 MB/s eta 0:00:06\n","   ---------------------------------- ----- 172.2/199.4 MB 4.9 MB/s eta 0:00:06\n","   ---------------------------------- ----- 173.0/199.4 MB 4.9 MB/s eta 0:00:06\n","   ---------------------------------- ----- 173.5/199.4 MB 4.9 MB/s eta 0:00:06\n","   ---------------------------------- ----- 174.3/199.4 MB 4.9 MB/s eta 0:00:06\n","   ----------------------------------- ---- 175.4/199.4 MB 4.8 MB/s eta 0:00:05\n","   ----------------------------------- ---- 175.9/199.4 MB 4.8 MB/s eta 0:00:05\n","   ----------------------------------- ---- 176.9/199.4 MB 4.8 MB/s eta 0:00:05\n","   ----------------------------------- ---- 177.5/199.4 MB 4.8 MB/s eta 0:00:05\n","   ----------------------------------- ---- 178.0/199.4 MB 4.8 MB/s eta 0:00:05\n","   ----------------------------------- ---- 179.0/199.4 MB 4.8 MB/s eta 0:00:05\n","   ------------------------------------ --- 179.8/199.4 MB 4.7 MB/s eta 0:00:05\n","   ------------------------------------ --- 180.6/199.4 MB 4.7 MB/s eta 0:00:04\n","   ------------------------------------ --- 181.4/199.4 MB 4.7 MB/s eta 0:00:04\n","   ------------------------------------ --- 182.5/199.4 MB 4.7 MB/s eta 0:00:04\n","   ------------------------------------ --- 183.5/199.4 MB 4.7 MB/s eta 0:00:04\n","   ------------------------------------- -- 184.5/199.4 MB 4.7 MB/s eta 0:00:04\n","   ------------------------------------- -- 185.3/199.4 MB 4.7 MB/s eta 0:00:03\n","   ------------------------------------- -- 186.1/199.4 MB 4.7 MB/s eta 0:00:03\n","   ------------------------------------- -- 186.6/199.4 MB 4.7 MB/s eta 0:00:03\n","   ------------------------------------- -- 187.7/199.4 MB 4.7 MB/s eta 0:00:03\n","   ------------------------------------- -- 188.2/199.4 MB 4.7 MB/s eta 0:00:03\n","   ------------------------------------- -- 189.0/199.4 MB 4.7 MB/s eta 0:00:03\n","   -------------------------------------- - 189.5/199.4 MB 4.7 MB/s eta 0:00:03\n","   -------------------------------------- - 190.3/199.4 MB 4.7 MB/s eta 0:00:02\n","   -------------------------------------- - 190.8/199.4 MB 4.7 MB/s eta 0:00:02\n","   -------------------------------------- - 191.6/199.4 MB 4.7 MB/s eta 0:00:02\n","   -------------------------------------- - 192.4/199.4 MB 4.6 MB/s eta 0:00:02\n","   -------------------------------------- - 193.2/199.4 MB 4.7 MB/s eta 0:00:02\n","   -------------------------------------- - 193.7/199.4 MB 4.6 MB/s eta 0:00:02\n","   ---------------------------------------  194.5/199.4 MB 4.6 MB/s eta 0:00:02\n","   ---------------------------------------  195.3/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------  196.3/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------  197.1/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------  197.9/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------  198.4/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------  199.2/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------  199.2/199.4 MB 4.6 MB/s eta 0:00:01\n","   ---------------------------------------- 199.4/199.4 MB 4.5 MB/s eta 0:00:00\n","Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n","Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n","Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n","Using cached idna-3.8-py3-none-any.whl (66 kB)\n","Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n","Using cached filelock-3.16.0-py3-none-any.whl (16 kB)\n","Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n","   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n","   ---------- ----------------------------- 0.5/2.1 MB 2.8 MB/s eta 0:00:01\n","   ------------------------- -------------- 1.3/2.1 MB 3.2 MB/s eta 0:00:01\n","   ------------------------------ --------- 1.6/2.1 MB 3.2 MB/s eta 0:00:01\n","   ---------------------------------------- 2.1/2.1 MB 2.8 MB/s eta 0:00:00\n","Using cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n","Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n","Installing collected packages: sentencepiece, mpmath, urllib3, tqdm, sympy, networkx, idna, fsspec, filelock, charset-normalizer, certifi, torch, requests, torchtext\n","Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.0 fsspec-2024.9.0 idna-3.8 mpmath-1.3.0 networkx-3.1 requests-2.32.3 sentencepiece-0.2.0 sympy-1.13.2 torch-2.4.1 torchtext-0.6.0 tqdm-4.66.5 urllib3-2.2.2\n"]}],"source":["!pip install -U torchtext==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPKSSHzQ6Uoh"},"outputs":[],"source":["!python -m spacy download en\n","!python -m spacy download de"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJWiAS2yWutF"},"outputs":[],"source":["import numpy as np\n","import random\n","import time\n","import math\n","import spacy\n","from torchtext.datasets import TranslationDataset\n","from torchtext.data import Field, BucketIterator\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"uHmIKputmJfU"},"source":["### Tokenizers\n","\n","- 문장의 토큰화, 태깅 등의 전처리를 수행하기 위해 `spaCy` 라이브러리에서 영어와 독일어 전처리 모듈을 설치해줍니다.\n","- 두 언어의 문장이 주어졌기 때문에 영어와 독일어 각각에 대해 전처리해주어야 합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7ZtI5IXm7EG"},"outputs":[],"source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"co1TC8yv7yrX"},"outputs":[],"source":["# 예시\n","result = spacy_en.tokenizer(\"I am a student.\")\n","\n","for i, token in enumerate(result):\n","    print(f\"인덱스 {i}: {token.text}\")"]},{"cell_type":"markdown","metadata":{"id":"lEGmP9Uk8gQG"},"source":["필드(field) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMjoI1XE7tGF"},"outputs":[],"source":["#===================================================\n","# 💡 토큰화 결과가 list로 반환될 수 있도록 return 결과값을 채워주세요\n","# seq2sxeq 논문에 의하면, input 단어의 순서를 바꾸면 최적화가 더 쉬워져 성능이 좋아진다고 합니다.\n","# 💡 독일어 토큰화 결과가 역순으로 return될 수 있도록 반영해주세요!\n","#===================================================\n","def tokenize_de(text):\n","    tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")  \n","    return tokenizer(text)[::-1] \n","\n","def tokenize_en(text):\n","    tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")  \n","    return tokenizer(text) "]},{"cell_type":"markdown","metadata":{"id":"z3wGw1nPnpMd"},"source":["필드(field) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubKI59GPpQ1f"},"outputs":[],"source":["# 독일어\n","SRC = Field(tokenize= tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True)\n","# 영어\n","TRG = Field(tokenize= tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True)"]},{"cell_type":"markdown","metadata":{"id":"0_ccI6_-8hR3"},"source":["### 데이터 불러오기\n","\n","대표적인 영어-독어 번역 데이터셋 Multi30k을 불러옵니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLA5kXAAf2uw"},"outputs":[],"source":["!git clone https://github.com/multi30k/dataset.git\n","\n","# 압축해제\n","!gunzip /content/dataset/data/task1/raw/train.de.gz\n","!gunzip /content/dataset/data/task1/raw/train.en.gz\n","!gunzip /content/dataset/data/task1/raw/val.de.gz\n","!gunzip /content/dataset/data/task1/raw/val.en.gz\n","!gunzip /content/dataset/data/task1/raw/test_2018_flickr.de.gz\n","!gunzip /content/dataset/data/task1/raw/test_2018_flickr.en.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0JchGVU91Q5"},"outputs":[],"source":["data_path = '/content/dataset/data/task1/raw/'\n","\n","train_data = TranslationDataset(path=data_path, exts=('train.de', 'train.en'), fields=(SRC, TRG) )\n","val_data = TranslationDataset(path=data_path, exts=('val.de', 'val.en'), fields=(SRC, TRG) )\n","test_data = TranslationDataset(path=data_path, exts=('test_2018_flickr.de', 'test_2018_flickr.en'), fields=(SRC, TRG) )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724602161494,"user":{"displayName":"차민재","userId":"05174396098945723154"},"user_tz":-540},"id":"qxLylN1y-urW","outputId":"62447416-c1f5-48c5-dec7-4c438c67a7f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["학습 데이터셋(training dataset) 크기: 29000개\n","평가 데이터셋(validation dataset) 크기: 1014개\n","테스트 데이터셋(testing dataset) 크기: 1071개\n"]}],"source":["print(f\"학습 데이터셋(training dataset) 크기: {len(train_data.examples)}개\")\n","print(f\"평가 데이터셋(validation dataset) 크기: {len(val_data.examples)}개\")\n","print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_data.examples)}개\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1724602163967,"user":{"displayName":"차민재","userId":"05174396098945723154"},"user_tz":-540},"id":"Rpt6l_Xd_AQX","outputId":"f8d29c4a-bef3-4d61-8175-fc08ca298369"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n","{'src': ['.', 'antriebsradsystem', 'ein', 'bedienen', 'schutzhelmen', 'mit', 'männer', 'mehrere'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"]}],"source":["print(vars(train_data.examples[0]))\n","print(vars(train_data.examples[1]))"]},{"cell_type":"markdown","metadata":{"id":"uNoigj40AD_0"},"source":["- `build_vocab`함수를 이용하여 영어와 독일어의 단어 사전을 생성해줍니다. 이를 통해 각 token이 indexing됩니다\n","- 단, vocabulary는 훈련 데이터셋에 대해서만 만들어져야 합니다.\n","- `min_freq`를 사용하여 최소 2번 이상 나오는 단어들만 사전에 포함되도록 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfK-pAr_ApLP"},"outputs":[],"source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyaA5P0Mrqaa"},"outputs":[],"source":["print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n","print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n","print(TRG.vocab.stoi[\"\"]) # : 0\n","print(TRG.vocab.stoi[\"\"]) # : 0\n","print(TRG.vocab.stoi[\"hello\"])\n","print(TRG.vocab.stoi[\"world\"])"]},{"cell_type":"markdown","metadata":{"id":"GmHXb1phBXJN"},"source":["- 시퀀스 데이터는 각 문장의 길이가 다를 수 있습니다.\n","- `BucketIterator는` 유사한 길이를 가진 샘플들을 같은 배치에 묶어주는 역할을 하기 때문에, 고정된 길이로 맞추기 위한 패딩의 양을 최소화할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uazI6xuv8rDH"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, val_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    device = device\n",")"]},{"cell_type":"markdown","metadata":{"id":"Z154iai8Czsr"},"source":["- 첫 번째 배치를 출력한 결과, [sequence length, batch size]라는 tensor가 생성됩니다\n","- `sequence length`는 해당 배치 내에서 가장 긴 문장의 길이를 의미하며, 이보다 짧은 문장은 <pad> token으로 채워집니다.\n","- 편의상 transpose한 뒤, 첫 번째와 두 번째 문장의 텐서를 출력하면, 특정 단어에 대응하는 인덱스가 출력되는 것을 알 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r-eReL8rwm_"},"outputs":[],"source":["for i, batch in enumerate(train_iterator):\n","    src = batch.src\n","    trg = batch.trg\n","\n","    print(f\"첫 번째 배치의 text 크기: {src.shape}\")\n","    src = src.transpose(1,0)\n","    print(src[0])\n","    print(src[1])\n","\n","    break"]},{"cell_type":"markdown","metadata":{"id":"51xSGy35XLvG"},"source":["### Building the Seq2Seq with LSTM Model\n","\n","- seq2seq 이해를 위한 과제이니, 아래를 참고하여 작성해도 무방합니다 :)\n","\n","\n","https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Sequence_to_Sequence_with_LSTM_Tutorial.ipynb"]},{"cell_type":"markdown","metadata":{"id":"i9WWS97vYnSb"},"source":["### Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtpU_ZjeYNEZ"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, p):\n","        super().__init__()\n","\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.dropout = nn.Dropout(p)\n","\n","        #=========================================#\n","        # 💡아래줄에 embedding과 multi-layer LSTM 부분을 채워주세요 (dropout 포함)\n","        #=========================================#\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=p)\n","\n","    def forward(self, x):\n","        # x = [x length, batch size]\n","        embedding = self.dropout(self.embedding(x))  # embedding = [x length, batch size, emb size]\n","\n","        outputs, (hidden, cell) = self.rnn(embedding)\n","\n","        # hidden = [n layers, batch size, hid dim]\n","        # cell = [n layer, batch size, hid dim]\n","        # outputs = [src len, batch size, hid dim]\n","\n","        return hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"ZrQoLPg-Ype1"},"source":["### Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOVQfminYknh"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, p):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.dropout = nn.Dropout(p)\n","\n","        #=========================================#\n","        # 💡아래 코드를 채워주고, 각각 어떤 역할을 하는지 주석으로 간단히 설명해주세요\n","        #\n","        #=========================================#\n","        self.embedding = nn.Embedding(output_dim, emb_dim) # embedding: 단어를 vector로 변환\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=p)  # RNN(LSTM): hidden state, cell state -> 다음 state 예측\n","        self.fc = nn.Linear(hid_dim, output_dim) # RC: 다음 단어 예측\n","\n","    def forward(self, input, hidden, cell):\n","\n","        # 현재 input 형태 = [batch size]\n","        # Decoder는 한번에 하나의 토큰만 처리하도록 sequence length = 1이 되어야 합니다\n","        input = input.unsqueeze(0)\n","\n","        embedding = self.dropout(self.embedding(input))\n","\n","        #=========================================#\n","        # 💡self.rnn() 괄호 안 부분을 채워주세요\n","        #=========================================#\n","        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n","\n","        prediction = self.fc(output.squeeze(0))  #prediction = [batch size, output dim]\n","\n","        return prediction, hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"NNsTqRamcfPg"},"source":["### Seq2Seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik28Umx6gAPW"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","        assert encoder.hid_dim == decoder.hid_dim, \\\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\n","        assert encoder.n_layers == decoder.n_layers, \\\n","            \"Encoder and decoder must have equal number of layers!\"\n","\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","\n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n","\n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        #=========================================#\n","        # 💡trg를 사용하여 decoder에 입력할 첫번째 input을 설정해주세요\n","        #=========================================#\n","        input = trg[0, :]\n","\n","        for t in range(1, trg_len):\n","\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            outputs[t] = output\n","\n","            # predictions들 중에 가장 잘 예측된 token 추출\n","            best_guess = output.argmax(1) # [batch size]\n","\n","            input = trg[t] if random.random() < teacher_forcing_ratio else best_guess\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"bfKrIZ63jkVF"},"source":["### **Q. 위 코드에서는 매 시점마다 확률이 가장 높은 다음 단어를 선택하는 Greedy decoding  방식이 사용됩니다. 이런 방법을 채택할 경우 발생할 수 있는 문제점은 무엇일지 작성해주세요.**\n","\n","\n","```python\n","\n","# predictions들 중에 가장 잘 예측된 token 추출\n","best_guess = output.argmax(1) # [batch size]\n","\n","```\n","\n","\n","➡️"]},{"cell_type":"markdown","metadata":{"id":"EHk36-bkh055"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xj33q4Izh3aB"},"outputs":[],"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"]},{"cell_type":"markdown","metadata":{"id":"uoFprkiyh5vm"},"source":["모델 초기 가중치 값은 논문의 내용대로 U(−0.08,0.08)의 연속균등분포로부터 얻습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oLZ0jXyh4zG"},"outputs":[],"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpKWiiBIF5NA"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","\n","# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"]},{"cell_type":"markdown","metadata":{"id":"veJrgUHBjRVa"},"source":["### Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDfEPQUwFvVG"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","\n","    model.train()\n","    epoch_loss = 0\n","\n","    for i, batch in enumerate(iterator):\n","\n","        src = batch.src\n","        trg = batch.trg\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        output_dim = output.shape[-1]\n","\n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0fOgC3jc8K4"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","\n","    model.eval()\n","\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","\n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0)\n","\n","            output_dim = output.shape[-1]\n","\n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OCHWUhmGORD"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0VHYyZLjFO2"},"outputs":[],"source":["N_EPOCHS = 3\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCou2VSKVz64"},"outputs":[],"source":["model.load_state_dict(torch.load('/content/seq2seq-lstm-model.pt'))\n","test_loss = evaluate(model, test_iterator, criterion)\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPO048WnZCYZh+CX7AzYDsd","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
